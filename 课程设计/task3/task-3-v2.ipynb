{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14133209,"sourceType":"datasetVersion","datasetId":9005841}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport os\nimport time\nfrom torchvision import models, transforms\nfrom torchvision.models import ResNet34_Weights","metadata":{"_uuid":"3bd58f99-8dee-453e-9fa7-315a40c67097","_cell_guid":"6b5feae9-4dd1-4a29-999b-55390eb6dafb","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-15T12:12:25.022410Z","iopub.execute_input":"2025-12-15T12:12:25.022640Z","iopub.status.idle":"2025-12-15T12:12:33.865276Z","shell.execute_reply.started":"2025-12-15T12:12:25.022617Z","shell.execute_reply":"2025-12-15T12:12:33.864656Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================\n# å®‰å…¨çŠ¶æ€å­—å…¸åŠ è½½å‡½æ•°\n# ======================\ndef load_state_dict_safely(model, state_dict):\n    \"\"\"æ™ºèƒ½å¤„ç† DataParallel å’Œæ™®é€šæ¨¡å‹çš„é”®åé—®é¢˜\"\"\"\n    model_state = model.state_dict()\n    \n    # æ ‡å‡†åŒ–é”®åï¼šç§»é™¤æ‰€æœ‰å¯èƒ½çš„å‰ç¼€\n    clean_state_dict = {}\n    for k, v in state_dict.items():\n        # ç§»é™¤ module. å’Œ _orig_mod. å‰ç¼€\n        clean_k = k.replace(\"module.\", \"\").replace(\"_orig_mod.\", \"\")\n        clean_state_dict[clean_k] = v\n    \n    # åˆ›å»ºåŒ¹é…çš„ state_dict\n    matched_state_dict = {}\n    for k in model_state.keys():\n        clean_k = k.replace(\"module.\", \"\").replace(\"_orig_mod.\", \"\")\n        if clean_k in clean_state_dict:\n            matched_state_dict[k] = clean_state_dict[clean_k]\n        else:\n            print(f\"âš ï¸ è­¦å‘Š: é”® '{k}' åœ¨state_dictä¸­ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–\")\n    \n    model.load_state_dict(matched_state_dict, strict=False)\n    return model","metadata":{"_uuid":"5ea2e851-e10c-43e7-a390-5e7e1783686b","_cell_guid":"78fc3fc2-46fd-4120-8a46-f8e82b9f6762","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-15T12:45:33.643087Z","iopub.execute_input":"2025-12-15T12:45:33.643971Z","iopub.status.idle":"2025-12-15T12:45:33.649249Z","shell.execute_reply.started":"2025-12-15T12:45:33.643937Z","shell.execute_reply":"2025-12-15T12:45:33.648494Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# ======================\n# æ•°æ®é›†å®šä¹‰\n# ======================\nclass FERDataset(Dataset):\n    def __init__(self, image_dir, mode='train', transform=None):\n        self.image_dir = Path(image_dir)\n        self.mode = mode\n        self.transform = transform\n        self.samples = []\n        self.class_to_idx = {\n            \"Angry\": 0, \"Fear\": 1, \"Happy\": 2, \n            \"Sad\": 3, \"Surprise\": 4, \"Neutral\": 5\n        }\n        self._load_data()\n\n    def _load_data(self):\n        if self.mode == \"train\":\n            for emotion_name, label in self.class_to_idx.items():\n                emotion_dir = self.image_dir / emotion_name\n                if emotion_dir.exists():\n                    for img_file in emotion_dir.glob(\"*\"):\n                        if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n                            self.samples.append((str(img_file), label))\n        else:\n            for img_file in self.image_dir.glob(\"*\"):\n                if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n                    self.samples.append((str(img_file), -1))\n\n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        if self.mode == 'test':\n            return image, img_path\n        else:\n            return image, label","metadata":{"_uuid":"f5dbdf52-9b23-485d-a6a1-783dfe6328b3","_cell_guid":"3a93a7d7-d15a-4c9f-bd55-88236c5ef5fe","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-15T12:45:37.931187Z","iopub.execute_input":"2025-12-15T12:45:37.931972Z","iopub.status.idle":"2025-12-15T12:45:37.938814Z","shell.execute_reply.started":"2025-12-15T12:45:37.931943Z","shell.execute_reply":"2025-12-15T12:45:37.938122Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, gamma=2.0, alpha=None):\n        super().__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n\n    def forward(self, inputs, targets):\n        ce = F.cross_entropy(\n            inputs, targets,\n            reduction='none',\n            weight=self.alpha\n        )\n        pt = torch.exp(-ce)\n        loss = ((1 - pt) ** self.gamma) * ce\n        return loss.mean()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def mixup_data(x, y, alpha=0.4):\n    lam = np.random.beta(alpha, alpha)\n    index = torch.randperm(x.size(0)).to(x.device)\n    mixed_x = lam * x + (1 - lam) * x[index]\n    return mixed_x, y, y[index], lam\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef tta_predict(model, images):\n    preds = []\n    preds.append(model(images))\n    preds.append(model(torch.flip(images, dims=[3])))        # æ°´å¹³ç¿»è½¬\n    preds.append(model(torch.rot90(images, 1, [2, 3])))      # æ—‹è½¬\n    return torch.mean(torch.stack(preds), dim=0)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================\n# æ¨¡å‹å®šä¹‰\n# ======================\nclass SEBlock(nn.Module):\n    def __init__(self, channel, reduction=8):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\nclass AdvancedCNN(nn.Module):\n    def __init__(self, out_channels=6, dropout_rate=0.4, pretrained=True):\n        super().__init__()\n        weights = ResNet34_Weights.IMAGENET1K_V1 if pretrained else None\n        self.backbone = models.resnet34(weights=weights)\n        self.se = SEBlock(512, reduction=8)\n        \n        # æ›¿æ¢åˆ†ç±»å¤´\n        self.backbone.fc = nn.Sequential(\n            nn.Dropout(dropout_rate),\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout_rate),\n            nn.Linear(256, out_channels)\n        )\n        \n    def forward(self, x):\n        x = self.backbone.conv1(x)\n        x = self.backbone.bn1(x)\n        x = self.backbone.relu(x)\n        x = self.backbone.maxpool(x)\n        x = self.backbone.layer1(x)\n        x = self.backbone.layer2(x)\n        x = self.backbone.layer3(x)\n        x = self.backbone.layer4(x)\n        x = self.se(x)\n        x = self.backbone.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.backbone.fc(x)\n        return x","metadata":{"_uuid":"0b67b07e-d0cc-4be0-a09e-33959efa493b","_cell_guid":"e622b0a1-ffeb-40f3-bf09-1da81b16e164","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-15T12:45:43.169768Z","iopub.execute_input":"2025-12-15T12:45:43.170331Z","iopub.status.idle":"2025-12-15T12:45:43.179450Z","shell.execute_reply.started":"2025-12-15T12:45:43.170293Z","shell.execute_reply":"2025-12-15T12:45:43.178716Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# ======================\n# æ•°æ®å¢å¼º\n# ======================\ndef create_transforms():\n    train_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomRotation(degrees=15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    val_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    return train_transform, val_transform","metadata":{"_uuid":"a1f36157-f17b-4a62-a41e-9f832ea3e0c8","_cell_guid":"27496030-631f-4be7-9fbc-3080ac6d8aa7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-15T12:45:48.412735Z","iopub.execute_input":"2025-12-15T12:45:48.413268Z","iopub.status.idle":"2025-12-15T12:45:48.418835Z","shell.execute_reply.started":"2025-12-15T12:45:48.413246Z","shell.execute_reply":"2025-12-15T12:45:48.418034Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# ======================\n# éªŒè¯å‡½æ•°ï¼ˆä¿®å¤ AMP è­¦å‘Šï¼‰\n# ======================\n@torch.no_grad()\ndef validate(model, val_loader, criterion, device):\n    model.eval()\n    total_loss, correct, total = 0.0, 0, 0\n    device_type = 'cuda' if device.type == 'cuda' else 'cpu'\n    \n    for images, labels in val_loader:\n        images, labels = images.to(device), labels.to(device)\n        with torch.amp.autocast(device_type=device_type):\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n        \n        total_loss += loss.item()\n        _, pred = outputs.max(1)\n        total += labels.size(0)\n        correct += pred.eq(labels).sum().item()\n        \n    return total_loss / len(val_loader), 100. * correct / total","metadata":{"_uuid":"5a90c5c5-7c3b-4874-b121-b11f91531909","_cell_guid":"06d12eca-b96c-40eb-9b71-4bf493e32b9f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-15T12:45:53.742847Z","iopub.execute_input":"2025-12-15T12:45:53.743637Z","iopub.status.idle":"2025-12-15T12:45:53.749347Z","shell.execute_reply.started":"2025-12-15T12:45:53.743605Z","shell.execute_reply":"2025-12-15T12:45:53.748653Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# # ======================\n# # è®­ç»ƒå‡½æ•°ï¼ˆä¿®å¤æ‰€æœ‰è­¦å‘Š + å®‰å…¨åŠ è½½ï¼‰\n# # ======================\n# def train_model(model, train_loader, val_loader, optimizer, criterion, \n#                 epochs=25, device=None):\n#     train_losses, val_losses, train_accs, val_accs = [], [], [], []\n    \n#     # âœ… ä¿®å¤ FutureWarning: ä½¿ç”¨æ–° API\n#     device_type = 'cuda' if device.type == 'cuda' else 'cpu'\n#     try:\n#         scaler = torch.amp.GradScaler(device_type)\n#     except (AttributeError, TypeError):\n#         # å…¼å®¹æ—§ç‰ˆæœ¬\n#         scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n    \n#     scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-7)\n#     best_val_acc = 0.0\n#     best_model_path = \"best_model.pth\"\n\n#     for epoch in range(epochs):\n#         model.train()\n#         total_loss, correct, total = 0.0, 0, 0\n#         start_time = time.time()\n        \n#         for images, labels in train_loader:\n#             images, labels = images.to(device), labels.to(device)\n#             optimizer.zero_grad()\n            \n#             # âœ… ä¿®å¤ FutureWarning: ä½¿ç”¨æ–° API\n#             with torch.amp.autocast(device_type=device_type):\n#                 outputs = model(images)\n#                 loss = criterion(outputs, labels)\n            \n#             if device.type == 'cuda':\n#                 scaler.scale(loss).backward()\n#                 scaler.step(optimizer)\n#                 scaler.update()\n#             else:\n#                 loss.backward()\n#                 optimizer.step()\n            \n#             total_loss += loss.item()\n#             _, pred = outputs.max(1)\n#             total += labels.size(0)\n#             correct += pred.eq(labels).sum().item()\n        \n#         epoch_time = time.time() - start_time\n#         scheduler.step()\n#         train_loss = total_loss / len(train_loader)\n#         train_acc = 100. * correct / total\n#         train_losses.append(train_loss)\n#         train_accs.append(train_acc)\n\n#         # Validation\n#         val_loss, val_acc = validate(model, val_loader, criterion, device)\n#         val_losses.append(val_loss)\n#         val_accs.append(val_acc)\n\n#         # æ‰“å°è¯¦ç»†æŒ‡æ ‡\n#         current_lr = scheduler.get_last_lr()[0]\n#         print(f\"Epoch {epoch+1:02d}/{epochs} | Time: {epoch_time:.1f}s | LR: {current_lr:.2e} | \"\n#               f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.2f}% | \"\n#               f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.2f}%\")\n\n#         # ä»…å½“éªŒè¯å‡†ç¡®ç‡æå‡æ—¶ä¿å­˜æ¨¡å‹\n#         if val_acc > best_val_acc:\n#             best_val_acc = val_acc\n#             if Path(best_model_path).exists():\n#                 Path(best_model_path).unlink()\n            \n#             # ä¿å­˜åŸå§‹state_dict\n#             state_dict = model.state_dict()\n#             # âœ… ä»…ä¿å­˜ tensorï¼Œé¿å… numpy æ ‡é‡\n#             torch.save({\"state_dict\": state_dict}, best_model_path)\n#             print(f\"  âœ… New best model saved @ {val_acc:.2f}%\")\n\n#     # ç»˜å›¾\n#     plot_training_history(train_losses, val_losses, train_accs, val_accs)\n    \n#     # æ ¸å¿ƒä¿®å¤ï¼šå®‰å…¨åŠ è½½æœ€ä¼˜æ¨¡å‹\n#     print(\"\\nğŸ”§ å®‰å…¨åŠ è½½æœ€ä¼˜æ¨¡å‹...\")\n#     checkpoint = torch.load(best_model_path, map_location=device, weights_only=True)\n#     model = load_state_dict_safely(model, checkpoint[\"state_dict\"])\n#     return best_model_path, model","metadata":{"_uuid":"9a80146a-447c-4a96-a03d-b5589088c408","_cell_guid":"95acd720-d306-410a-923f-3b04adaabdbe","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-15T12:46:07.907063Z","iopub.execute_input":"2025-12-15T12:46:07.907311Z","iopub.status.idle":"2025-12-15T12:46:07.917767Z","shell.execute_reply.started":"2025-12-15T12:46:07.907293Z","shell.execute_reply":"2025-12-15T12:46:07.916923Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# # ======================\n# # æµ‹è¯• & æäº¤\n# # ======================\n# def test(model, test_loader, device, output_csv='submission.csv'):\n#     model.eval()\n#     paths, labels = [], []\n#     with torch.no_grad():\n#         for images, path_list in test_loader:\n#             images = images.to(device)\n#             outputs = model(images)\n#             _, pred = outputs.max(1)\n#             labels.extend(pred.cpu().numpy().tolist())\n#             if isinstance(path_list, (list, tuple)):\n#                 file_names = [Path(p).name for p in path_list]\n#             else:\n#                 file_names = [Path(path_list).name]\n#             paths.extend(file_names)\n    \n#     df = pd.DataFrame({\"ID\": paths, \"Emotion\": labels})\n#     df.to_csv(output_csv, index=False)\n#     print(f\"âœ… Submission saved to '{output_csv}' (rows: {len(df)})\")","metadata":{"_uuid":"f8f261ed-4c50-44ab-92ef-e76f541245f9","_cell_guid":"f538fd3f-bdb6-44a9-85bb-bb7646f6cc7d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-15T12:46:12.242728Z","iopub.execute_input":"2025-12-15T12:46:12.243597Z","iopub.status.idle":"2025-12-15T12:46:12.249503Z","shell.execute_reply.started":"2025-12-15T12:46:12.243569Z","shell.execute_reply":"2025-12-15T12:46:12.248636Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# ======================\n# å¯è§†åŒ–\n# ======================\ndef plot_training_history(train_losses, val_losses, train_accs, val_accs):\n    plt.style.use('seaborn-v0_8-whitegrid')\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    epochs = range(1, len(train_losses) + 1)\n    \n    # Loss\n    ax1.plot(epochs, train_losses, 'b-o', label='Train Loss', linewidth=2, markersize=4)\n    ax1.plot(epochs, val_losses, 'r-s', label='Val Loss', linewidth=2, markersize=4)\n    ax1.set_xlabel('Epoch', fontsize=12)\n    ax1.set_ylabel('Loss', fontsize=12)\n    ax1.set_title('Loss Curve', fontsize=14)\n    ax1.legend(loc='best')\n    ax1.grid(alpha=0.3)\n    \n    # Accuracy\n    ax2.plot(epochs, train_accs, 'b-o', label='Train Accuracy', linewidth=2, markersize=4)\n    ax2.plot(epochs, val_accs, 'r-s', label='Val Accuracy', linewidth=2, markersize=4)\n    \n    # æ ‡è®°æœ€ä½³ç‚¹\n    best_epoch = val_accs.index(max(val_accs)) + 1\n    best_val = max(val_accs)\n    ax2.scatter(best_epoch, best_val, color='gold', edgecolor='black', s=150, zorder=5,\n                label=f'Best: {best_val:.2f}% @ Epoch {best_epoch}')\n    \n    ax2.set_xlabel('Epoch', fontsize=12)\n    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n    ax2.set_title('Accuracy Curve', fontsize=14)\n    ax2.legend(loc='best')\n    ax2.grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(\"training_history.png\", dpi=300, bbox_inches='tight')\n    plt.close()\n    print(\"ğŸ“ˆ Training history saved as 'training_history.png'\")","metadata":{"_uuid":"a4758be8-75ce-4925-a48d-01da4a88f3f2","_cell_guid":"7b2d5115-1f05-4188-820b-24ff8d122f48","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-15T12:46:01.580495Z","iopub.execute_input":"2025-12-15T12:46:01.580805Z","iopub.status.idle":"2025-12-15T12:46:01.588115Z","shell.execute_reply.started":"2025-12-15T12:46:01.580781Z","shell.execute_reply":"2025-12-15T12:46:01.587516Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, optimizer, criterion,\n                epochs=25, device=None):\n\n    device_type = 'cuda' if device.type == 'cuda' else 'cpu'\n    scaler = torch.amp.GradScaler(device_type)\n\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=epochs, eta_min=1e-7\n    )\n\n    best_val_acc = 0.0\n    best_model_path = \"best_model.pth\"\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss, correct, total = 0.0, 0, 0\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            # ===== MixUp =====\n            images, y_a, y_b, lam = mixup_data(images, labels)\n\n            with torch.amp.autocast(device_type=device_type):\n                outputs = model(images)\n                loss = lam * criterion(outputs, y_a) + \\\n                       (1 - lam) * criterion(outputs, y_b)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            total_loss += loss.item()\n\n        scheduler.step()\n\n        # ===== Validation =====\n        val_loss, val_acc = validate(model, val_loader, criterion, device)\n\n        print(f\"Epoch [{epoch+1}/{epochs}] \"\n              f\"Train Loss: {total_loss/len(train_loader):.4f} | \"\n              f\"Val Acc: {val_acc:.2f}%\")\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\"state_dict\": model.state_dict()}, best_model_path)\n            print(f\"  âœ… Best model saved: {val_acc:.2f}%\")\n\n    checkpoint = torch.load(best_model_path, map_location=device)\n    model.load_state_dict(checkpoint[\"state_dict\"], strict=False)\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test(model, test_loader, device, output_csv='submission.csv'):\n    model.eval()\n    paths, labels = [], []\n\n    with torch.no_grad():\n        for images, path_list in test_loader:\n            images = images.to(device)\n\n            outputs = tta_predict(model, images)\n            _, pred = outputs.max(1)\n\n            labels.extend(pred.cpu().numpy().tolist())\n            paths.extend([Path(p).name for p in path_list])\n\n    df = pd.DataFrame({\"ID\": paths, \"Emotion\": labels})\n    df.to_csv(output_csv, index=False)\n    print(f\"âœ… Submission saved to {output_csv}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ======================\n# # ä¸»ç¨‹åºï¼ˆT4*2 ä¸“å±ï¼Œç¦ç”¨ torch.compile é¿å… DataParallel å†²çªï¼‰\n# # ======================\n# if __name__ == '__main__':\n#     # è®¾å¤‡è®¾ç½®\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     print(f\"âœ… Device: {device}\")\n#     print(f\"âœ… PyTorch version: {torch.__version__}\")\n    \n#     if torch.cuda.is_available():\n#         print(f\"  â†’ GPU count: {torch.cuda.device_count()}\")\n#         for i in range(torch.cuda.device_count()):\n#             print(f\"  â†’ GPU {i}: {torch.cuda.get_device_name(i)}\")\n    \n#     # è·¯å¾„é…ç½®\n#     ROOT_DIR = \"/kaggle/input/emotion-classfication/fer_data/fer_data\"\n#     if not os.path.exists(ROOT_DIR):\n#         print(f\"âŒ Path not found: {ROOT_DIR}\")\n#         # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å¯èƒ½è·¯å¾„\n#         candidates = [\n#             \"/kaggle/input/fer-data/fer_data\",\n#             \"/kaggle/input/fer2013/fer_data\",\n#             \"./fer_data\"\n#         ]\n#         for cand in candidates:\n#             if os.path.exists(cand):\n#                 ROOT_DIR = cand\n#                 print(f\"âœ… Found data at: {ROOT_DIR}\")\n#                 break\n    \n#     train_path = Path(ROOT_DIR) / \"train\"\n#     test_path = Path(ROOT_DIR) / \"test\"\n    \n#     # æ£€æŸ¥è·¯å¾„å­˜åœ¨æ€§\n#     if not train_path.exists():\n#         print(f\"âŒ Train path not found: {train_path}\")\n#         print(\"Available directories:\")\n#         for item in Path(ROOT_DIR).iterdir():\n#             print(f\"  - {item}\")\n#         raise FileNotFoundError(f\"Train directory not found at {train_path}\")\n    \n#     # åˆ›å»ºå˜æ¢\n#     train_transform, val_transform = create_transforms()\n    \n#     # åˆ›å»ºæ•°æ®é›†\n#     try:\n#         full_train_dataset = FERDataset(train_path, \"train\", transform=None)\n#         print(f\"âœ… Loaded {len(full_train_dataset)} training samples\")\n#     except Exception as e:\n#         print(f\"âŒ Error loading dataset: {e}\")\n#         raise\n    \n#     train_dataset = FERDataset(train_path, \"train\", transform=train_transform)\n#     val_dataset = FERDataset(train_path, \"train\", transform=val_transform)\n#     test_dataset = FERDataset(test_path, \"test\", transform=val_transform)\n    \n#     # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†\n#     total = len(full_train_dataset)\n#     val_ratio = 0.2\n#     val_size = int(total * val_ratio)\n#     train_size = total - val_size\n#     generator = torch.Generator().manual_seed(42)\n#     indices = torch.randperm(total, generator=generator).tolist()\n#     train_indices = indices[:train_size]\n#     val_indices = indices[train_size:]\n    \n#     train_subset = Subset(train_dataset, train_indices)\n#     val_subset = Subset(val_dataset, val_indices)\n    \n#     # T4*2 ä¸“å±é…ç½®\n#     if torch.cuda.device_count() > 1:\n#         BATCH_SIZE = 256  # åŒå¡\n#         print(f\"ğŸš€ Using 2x T4 GPUs with batch_size={BATCH_SIZE}\")\n#     else:\n#         BATCH_SIZE = 128  # å•å¡\n#         print(f\"ğŸš€ Using single GPU with batch_size={BATCH_SIZE}\")\n    \n#     NUM_WORKERS = min(4, os.cpu_count() or 4)  # å®‰å…¨å€¼\n    \n#     train_loader = DataLoader(\n#         train_subset, \n#         batch_size=BATCH_SIZE, \n#         shuffle=True,\n#         num_workers=NUM_WORKERS,\n#         pin_memory=True,\n#         persistent_workers=True,\n#         drop_last=True\n#     )\n#     val_loader = DataLoader(\n#         val_subset, \n#         batch_size=BATCH_SIZE, \n#         shuffle=False,\n#         num_workers=NUM_WORKERS,\n#         pin_memory=True,\n#         persistent_workers=True\n#     )\n#     test_loader = DataLoader(\n#         test_dataset, \n#         batch_size=BATCH_SIZE, \n#         shuffle=False,\n#         num_workers=NUM_WORKERS,\n#         pin_memory=True\n#     )\n    \n#     print(f\"ğŸ“Š Dataset sizes: Train={len(train_subset)}, Val={len(val_subset)}, Test={len(test_dataset)}\")\n    \n#     # æ¨¡å‹åˆå§‹åŒ–\n#     model = AdvancedCNN(pretrained=True).to(device)\n    \n#     # T4*2 å¤šå¡åŠ é€Ÿ (ä½¿ç”¨ DataParallelï¼Œç¦ç”¨ torch.compile)\n#     if torch.cuda.device_count() > 1:\n#         model = nn.DataParallel(model)\n#         print(f\"ParallelGroup: Using {torch.cuda.device_count()} GPUs\")\n#         # âœ… å…³é”®ï¼šä¸ä½¿ç”¨ torch.compileï¼Œé¿å… DataParallel å†²çª\n#         print(\"âš ï¸ torch.compile disabled for DataParallel compatibility\")\n#     else:\n#         # å•å¡æ—¶å¯å¯ç”¨ compile\n#         if hasattr(torch, 'compile') and torch.__version__ >= '2.0':\n#             try:\n#                 model = torch.compile(model, mode=\"reduce-overhead\")\n#                 print(\"âœ… torch.compile enabled (single GPU)\")\n#             except Exception as e:\n#                 print(f\"âš ï¸ torch.compile failed: {e}\")\n    \n#     # ä¼˜åŒ–å™¨ & æŸå¤±\n#     criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n#     optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4, betas=(0.9, 0.999))\n    \n#     # è®­ç»ƒ\n#     print(\"\\nğŸš€ Starting high-performance training...\")\n#     best_model_path, model = train_model(\n#         model, train_loader, val_loader, optimizer, criterion,\n#         epochs=25, device=device\n#     )\n    \n#     # æµ‹è¯•ï¼ˆç¡®ä¿ä½¿ç”¨å•å¡æ¨¡å‹ï¼‰\n#     if hasattr(model, \"module\"):\n#         # æå–å•å¡æ¨¡å‹ç”¨äºæµ‹è¯•\n#         test_model = AdvancedCNN().to(device)\n#         test_model.load_state_dict(model.module.state_dict())\n#         model = test_model\n    \n#     # æµ‹è¯•\n#     print(\"\\nğŸ§ª Running inference on test set...\")\n   \n\n#     test(model, test_loader, device, output_csv='submission.csv')","metadata":{"_uuid":"55e526ea-30dd-40fb-9c98-e31615151812","_cell_guid":"c8b637cc-1ff7-4731-8705-2c84ede7dd33","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-15T12:48:16.611719Z","iopub.execute_input":"2025-12-15T12:48:16.612247Z","iopub.status.idle":"2025-12-15T13:12:28.572101Z","shell.execute_reply.started":"2025-12-15T12:48:16.612225Z","shell.execute_reply":"2025-12-15T13:12:28.571276Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"âœ… Device: cuda\nâœ… PyTorch version: 2.6.0+cu124\n  â†’ GPU count: 2\n  â†’ GPU 0: Tesla T4\n  â†’ GPU 1: Tesla T4\nâœ… Loaded 28275 training samples\nğŸš€ Using 2x T4 GPUs with batch_size=256\nğŸ“Š Dataset sizes: Train=22620, Val=5655, Test=7065\nParallelGroup: Using 2 GPUs\nâš ï¸ torch.compile disabled for DataParallel compatibility\n\nğŸš€ Starting high-performance training...\nEpoch 01/25 | Time: 52.8s | LR: 2.99e-04 | Train Loss: 1.4619 | Acc: 45.53% | Val Loss: 1.2758 | Acc: 55.63%\n  âœ… New best model saved @ 55.63%\nEpoch 02/25 | Time: 50.5s | LR: 2.95e-04 | Train Loss: 1.2417 | Acc: 59.02% | Val Loss: 1.2111 | Acc: 60.21%\n  âœ… New best model saved @ 60.21%\nEpoch 03/25 | Time: 50.6s | LR: 2.89e-04 | Train Loss: 1.1708 | Acc: 62.73% | Val Loss: 1.1359 | Acc: 63.48%\n  âœ… New best model saved @ 63.48%\nEpoch 04/25 | Time: 50.5s | LR: 2.81e-04 | Train Loss: 1.1251 | Acc: 65.15% | Val Loss: 1.1368 | Acc: 63.87%\n  âœ… New best model saved @ 63.87%\nEpoch 05/25 | Time: 50.6s | LR: 2.71e-04 | Train Loss: 1.0856 | Acc: 67.44% | Val Loss: 1.1376 | Acc: 64.03%\n  âœ… New best model saved @ 64.03%\nEpoch 06/25 | Time: 50.4s | LR: 2.59e-04 | Train Loss: 1.0558 | Acc: 68.91% | Val Loss: 1.1369 | Acc: 64.90%\n  âœ… New best model saved @ 64.90%\nEpoch 07/25 | Time: 50.2s | LR: 2.46e-04 | Train Loss: 1.0235 | Acc: 71.00% | Val Loss: 1.1181 | Acc: 65.76%\n  âœ… New best model saved @ 65.76%\nEpoch 08/25 | Time: 50.2s | LR: 2.30e-04 | Train Loss: 0.9916 | Acc: 72.73% | Val Loss: 1.1109 | Acc: 65.84%\n  âœ… New best model saved @ 65.84%\nEpoch 09/25 | Time: 50.2s | LR: 2.14e-04 | Train Loss: 0.9614 | Acc: 74.20% | Val Loss: 1.1419 | Acc: 65.55%\nEpoch 10/25 | Time: 50.3s | LR: 1.96e-04 | Train Loss: 0.9235 | Acc: 75.95% | Val Loss: 1.1354 | Acc: 67.36%\n  âœ… New best model saved @ 67.36%\nEpoch 11/25 | Time: 50.5s | LR: 1.78e-04 | Train Loss: 0.8899 | Acc: 78.03% | Val Loss: 1.1528 | Acc: 67.18%\nEpoch 12/25 | Time: 50.3s | LR: 1.59e-04 | Train Loss: 0.8513 | Acc: 80.29% | Val Loss: 1.1417 | Acc: 66.60%\nEpoch 13/25 | Time: 50.2s | LR: 1.41e-04 | Train Loss: 0.8088 | Acc: 82.17% | Val Loss: 1.1673 | Acc: 66.42%\nEpoch 14/25 | Time: 50.5s | LR: 1.22e-04 | Train Loss: 0.7762 | Acc: 83.88% | Val Loss: 1.1724 | Acc: 66.74%\nEpoch 15/25 | Time: 51.0s | LR: 1.04e-04 | Train Loss: 0.7376 | Acc: 85.86% | Val Loss: 1.2290 | Acc: 66.72%\nEpoch 16/25 | Time: 51.3s | LR: 8.62e-05 | Train Loss: 0.7038 | Acc: 87.27% | Val Loss: 1.1887 | Acc: 66.63%\nEpoch 17/25 | Time: 50.9s | LR: 6.97e-05 | Train Loss: 0.6694 | Acc: 89.20% | Val Loss: 1.2242 | Acc: 66.93%\nEpoch 18/25 | Time: 50.4s | LR: 5.45e-05 | Train Loss: 0.6378 | Acc: 90.82% | Val Loss: 1.2385 | Acc: 67.37%\n  âœ… New best model saved @ 67.37%\nEpoch 19/25 | Time: 50.8s | LR: 4.07e-05 | Train Loss: 0.6034 | Acc: 92.37% | Val Loss: 1.2313 | Acc: 68.10%\n  âœ… New best model saved @ 68.10%\nEpoch 20/25 | Time: 50.6s | LR: 2.87e-05 | Train Loss: 0.5819 | Acc: 93.51% | Val Loss: 1.2430 | Acc: 67.96%\nEpoch 21/25 | Time: 51.4s | LR: 1.86e-05 | Train Loss: 0.5651 | Acc: 94.29% | Val Loss: 1.2452 | Acc: 67.96%\nEpoch 22/25 | Time: 51.6s | LR: 1.06e-05 | Train Loss: 0.5491 | Acc: 95.09% | Val Loss: 1.2585 | Acc: 68.01%\nEpoch 23/25 | Time: 50.8s | LR: 4.81e-06 | Train Loss: 0.5379 | Acc: 95.67% | Val Loss: 1.2559 | Acc: 68.31%\n  âœ… New best model saved @ 68.31%\nEpoch 24/25 | Time: 50.8s | LR: 1.28e-06 | Train Loss: 0.5386 | Acc: 95.70% | Val Loss: 1.2592 | Acc: 68.13%\nEpoch 25/25 | Time: 50.9s | LR: 1.00e-07 | Train Loss: 0.5335 | Acc: 95.86% | Val Loss: 1.2593 | Acc: 68.28%\nğŸ“ˆ Training history saved as 'training_history.png'\n\nğŸ”§ å®‰å…¨åŠ è½½æœ€ä¼˜æ¨¡å‹...\n\nğŸ§ª Running inference on test set...\nâœ… Submission saved to 'submission.csv' (rows: 7065)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"if __name__ == '__main__':\n    # ======================\n    # è®¾å¤‡\n    # ======================\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"âœ… Device: {device}\")\n    print(f\"âœ… PyTorch: {torch.__version__}\")\n\n    if torch.cuda.is_available():\n        print(f\"  â†’ GPU count: {torch.cuda.device_count()}\")\n        for i in range(torch.cuda.device_count()):\n            print(f\"  â†’ GPU {i}: {torch.cuda.get_device_name(i)}\")\n\n    # ======================\n    # æ•°æ®è·¯å¾„\n    # ======================\n    ROOT_DIR = \"/kaggle/input/emotion-classfication/fer_data/fer_data\"\n    train_path = Path(ROOT_DIR) / \"train\"\n    test_path  = Path(ROOT_DIR) / \"test\"\n\n    if not train_path.exists():\n        raise FileNotFoundError(f\"Train path not found: {train_path}\")\n\n    # ======================\n    # æ•°æ®å¢å¼º\n    # ======================\n    train_transform, val_transform = create_transforms()\n\n    # ======================\n    # æ•°æ®é›†\n    # ======================\n    full_train_dataset = FERDataset(train_path, \"train\", transform=None)\n    train_dataset = FERDataset(train_path, \"train\", transform=train_transform)\n    val_dataset   = FERDataset(train_path, \"train\", transform=val_transform)\n    test_dataset  = FERDataset(test_path,  \"test\",  transform=val_transform)\n\n    print(f\"âœ… Total train samples: {len(full_train_dataset)}\")\n\n    # ======================\n    # åˆ’åˆ† Train / Val\n    # ======================\n    total = len(full_train_dataset)\n    val_ratio = 0.2\n    val_size = int(total * val_ratio)\n    train_size = total - val_size\n\n    generator = torch.Generator().manual_seed(42)\n    indices = torch.randperm(total, generator=generator).tolist()\n    train_indices = indices[:train_size]\n    val_indices   = indices[train_size:]\n\n    train_subset = Subset(train_dataset, train_indices)\n    val_subset   = Subset(val_dataset, val_indices)\n\n    # ======================\n    # DataLoader\n    # ======================\n    if torch.cuda.device_count() > 1:\n        BATCH_SIZE = 256\n        print(\"ğŸš€ Using multi-GPU batch_size=256\")\n    else:\n        BATCH_SIZE = 128\n        print(\"ğŸš€ Using single-GPU batch_size=128\")\n\n    NUM_WORKERS = min(4, os.cpu_count() or 4)\n\n    train_loader = DataLoader(\n        train_subset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n        persistent_workers=True,\n        drop_last=True\n    )\n\n    val_loader = DataLoader(\n        val_subset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n        persistent_workers=True\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=True\n    )\n\n    print(f\"ğŸ“Š Train={len(train_subset)} | Val={len(val_subset)} | Test={len(test_dataset)}\")\n\n    # ======================\n    # æ¨¡å‹\n    # ======================\n    model = AdvancedCNN(pretrained=True).to(device)\n\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n        print(\"âš ï¸ DataParallel enabled (torch.compile disabled)\")\n    else:\n        if hasattr(torch, \"compile\") and torch.__version__ >= \"2.0\":\n            try:\n                model = torch.compile(model, mode=\"reduce-overhead\")\n                print(\"âœ… torch.compile enabled\")\n            except Exception as e:\n                print(f\"âš ï¸ torch.compile failed: {e}\")\n\n    # ======================\n    # â­ å…³é”®ä¿®æ”¹ â‘ ï¼šç±»åˆ«æƒé‡\n    # ======================\n    from collections import Counter\n\n    labels = [label for _, label in full_train_dataset.samples]\n    counter = Counter(labels)\n    print(\"ğŸ“Š Class distribution:\", counter)\n\n    class_weights = torch.tensor(\n        [1.0 / counter[i] for i in range(6)],\n        dtype=torch.float\n    )\n    class_weights = class_weights / class_weights.sum() * 6\n    class_weights = class_weights.to(device)\n\n    # ======================\n    # â­ å…³é”®ä¿®æ”¹ â‘¡ï¼šFocal Lossï¼ˆå¸¦æƒé‡ï¼‰\n    # ======================\n    criterion = FocalLoss(\n        gamma=2.0,\n        alpha=class_weights\n    )\n\n    # ï¼ˆå¦‚æœä½ æƒ³ç”¨ Weighted CEï¼Œæ”¹æˆä¸‹é¢è¿™è¡Œå³å¯ï¼‰\n    # criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.05)\n\n    # ======================\n    # ä¼˜åŒ–å™¨\n    # ======================\n    optimizer = optim.AdamW(\n        model.parameters(),\n        lr=3e-4,\n        weight_decay=1e-4,\n        betas=(0.9, 0.999)\n    )\n\n    # ======================\n    # è®­ç»ƒï¼ˆå†…éƒ¨å·²å¯ç”¨ MixUpï¼‰\n    # ======================\n    print(\"\\nğŸš€ Start Training (Focal + MixUp)...\")\n    model = train_model(\n        model,\n        train_loader,\n        val_loader,\n        optimizer,\n        criterion,\n        epochs=25,\n        device=device\n    )\n\n    # ======================\n    # å¤šå¡ â†’ å•å¡ï¼ˆæµ‹è¯•ç”¨ï¼‰\n    # ======================\n    if hasattr(model, \"module\"):\n        single_model = AdvancedCNN(pretrained=False).to(device)\n        single_model.load_state_dict(model.module.state_dict())\n        model = single_model\n\n    # ======================\n    # â­ å…³é”®ä¿®æ”¹ â‘¢ï¼šTTA æµ‹è¯•\n    # ======================\n    print(\"\\nğŸ§ª Inference with TTA...\")\n    test(\n        model,\n        test_loader,\n        device,\n        output_csv=\"submission.csv\"\n    )\n\n    print(\"\\nğŸ‰ Done! Focal + MixUp + TTA pipeline finished.\")\n","metadata":{"_uuid":"ecdb3ee8-edd3-4782-a001-855cd86092a2","_cell_guid":"a813257a-0164-4dc5-b5f7-3093bd67c3f0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-15T12:13:18.383032Z","iopub.execute_input":"2025-12-15T12:13:18.383730Z","iopub.status.idle":"2025-12-15T12:38:33.432053Z","shell.execute_reply.started":"2025-12-15T12:13:18.383705Z","shell.execute_reply":"2025-12-15T12:38:33.430840Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}